{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+-----------+-----------------+-------------+------------+------+-----------+\n",
      "|order_number|partition_order_number|rank_number|dense_rank_number|customer_name|product_name|amount|vendor_name|\n",
      "+------------+----------------------+-----------+-----------------+-------------+------------+------+-----------+\n",
      "|           1|                     2|          1|                1|     Rajendra|       Books| 119.0|    Archies|\n",
      "|           2|                     3|          1|                1|     Rajendra|        Pens| 302.0|    Archies|\n",
      "|           3|                     2|          3|                2|         Raju|         Bag|203.45|       Bata|\n",
      "|           4|                     1|          4|                3|         Shiv|       Shoes|100.24|       Bata|\n",
      "|           5|                     1|          5|                4|       Sukesh|     Perfume| 102.0|    Archies|\n",
      "+------------+----------------------+-----------+-----------------+-------------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, desc, rank, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()\n",
    "data = [['Shiv','Shoes',100.24,'Bata'],['Raju','Bag', 203.45,\"Bata\"],['Sukesh','Perfume', 102.00, 'Archies'],[\"Rajendra\", 'Books', 119.00, \"Archies\"],['Rajendra','Pens', 302.00, 'Archies']]\n",
    "schema = ['customer_name', 'product_name','amount', 'vendor_name']\n",
    "df  = spark.createDataFrame(data=data, schema=schema)\n",
    "df.createOrReplaceTempView('sales')\n",
    "\n",
    "sql normal select query\n",
    "df = spark.sql(\"select customer_name, product_name, amount, vendor_name from sales\")\n",
    "df.show()\n",
    "\n",
    "##########\n",
    "## Row Number:: row_number function used to generate unique number for each row depend on some specific column field\n",
    "##########\n",
    "\n",
    "#example of sql query on row_number built in function \n",
    "# df = spark.sql( 'select row_number() over(order by customer_name) as order_number, customer_name, product_name, amount, vendor_name from sales').show()\n",
    "\n",
    "#Example of pysaprk query on row_number built in function \n",
    "# df = df.select(row_number().over(Window.orderBy('customer_name')).alias('order_number'), 'customer_name', 'product_name', 'amount', 'vendor_name').show()\n",
    "\n",
    "\n",
    "###########\n",
    "## Partititon By:: Partition by used to partiton into groups depend on perticular column values & provide rank depend on individual group.\n",
    "###########\n",
    "\n",
    "#example of sql query on row_number & partition by build in function\n",
    "# df = spark.sql('select row_number() over(order by vendor_name) as order_number, row_number() over(partition by vendor_name order by vendor_name) as partition_order_number, customer_name, product_name, amount, vendor_name from sales').show()\n",
    "\n",
    "#example of pyspark query on row_number & partition by build in function\n",
    "# df = df.select(row_number().over(Window.orderBy('vendor_name')).alias('order_number'), row_number().over(Window.partitionBy('vendor_name').orderBy('vendor_name')).alias('partition_order_number'), 'customer_name', 'product_name', 'amount', 'vendor_name').show()\n",
    "\n",
    "##############\n",
    "### Rank:: Used to give the unique rank value for each row bas on perticular column field\n",
    "##############\n",
    "\n",
    "#Exaample of sql query on rank function\n",
    "# df = spark.sql('select row_number() over(order by customer_name) as row_number, row_number() over(partition by customer_name order by customer_name) as partition_row_number,rank() over(order by customer_name) as rank_number, customer_name, product_name, amount, vendor_name from sales').show()\n",
    "\n",
    "#Example of pyspark sql on rank function\n",
    "# df = df.select(row_number().over(Window.orderBy('customer_name')).alias('order_number'), row_number().over(Window.partitionBy('vendor_name').orderBy('vendor_name')).alias('partition_order_number'), rank().over(Window.orderBy('customer_name')).alias('rank_number'), 'customer_name', 'product_name', 'amount', 'vendor_name').show()\n",
    "\n",
    "\n",
    "#################\n",
    "## DenseRank:: Used to give the unique rank value for each diffenent value with out spikking the rank value sequnence\n",
    "#################\n",
    "\n",
    "#Exaample of sql query on rank function\n",
    "# df = spark.sql('select row_number() over(order by customer_name) as row_number, row_number() over(partition by customer_name order by customer_name) as partition_row_number,rank() over(order by customer_name) as rank_number, dense_rank() over(order by customer_name) as desne_rank_number,customer_name, product_name, amount, vendor_name from sales').show()\n",
    "\n",
    "#Example of pyspark sql on rank function\n",
    "df = df.select(row_number().over(Window.orderBy('customer_name')).alias('order_number'), row_number().over(Window.partitionBy('vendor_name').orderBy('vendor_name')).alias('partition_order_number'), rank().over(Window.orderBy('customer_name')).alias('rank_number'),dense_rank().over(Window.orderBy('customer_name')).alias('dense_rank_number'), 'customer_name', 'product_name', 'amount', 'vendor_name').show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
